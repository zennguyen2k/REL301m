Mục tiêu: 
- Hiểu cách sử dụng lập trình động để tính toán hàm giá trị và chính sách tối ưu trong các bài toán điều khiển công nghiệp.

- Áp dụng các thuật toán như Policy Iteration và Value Iteration để giải quyết các bài toán MDP (Markov Decision Process).

- Thực hành triển khai các thuật toán này trong môi trường mô phỏng.

Nội dung chính: 
1. Lập trình động trong học tăng cường
Sử dụng mô hình môi trường đã biết để tính toán chính sách tối ưu.
Áp dụng các thuật toán như Policy Iteration và Value Iteration để tìm chính sách tối ưu.
2. Thuật toán Policy Iteration
- Gồm hai bước lặp: 
Policy Evaluation: Tính toán hàm giá trị cho chính sách hiện tại.
Policy Improvement: Cập nhật chính sách dựa trên hàm giá trị đã tính.
=> Lặp lại hai bước trên cho đến khi chính sách hội tụ.
3. Thuật toán Value Iteration
Kết hợp đánh giá và cải thiện chính sách trong một bước duy nhất
Lặp lại cho đến khi hàm giá trị hội tụ.
4. Thực hành trong môi trường mô phỏng
Triển khai các thuật toán trên trong môi trường mô phỏng để giải quyết bài toán điều khiển công nghiệp.
Đánh giá hiệu quả của các thuật toán thông qua kết quả mô phỏng.

Kết luận: 
Áp dụng lý thuyết vào thực tế thông qua các bài tập thực hành.

Hiểu rõ cách sử dụng lập trình động trong các bài toán điều khiển công nghiệp.

Phát triển kỹ năng triển khai và đánh giá các thuật toán học tăng cường.





