- Mục tiêu: 
Hiểu cách mô hình hóa các bài toán quyết định tuần tự bằng Markov Decision Processes (MDPs).

Nắm vững các thành phần cơ bản của MDP và cách chúng tương tác với nhau.

Phân biệt giữa các loại nhiệm vụ trong học tăng cường: nhiệm vụ kết thúc (episodic) và nhiệm vụ liên tục (continuing).

- Nội dung chính: 
1. Định nghĩa Markov Decision Process (MDP)
MDP bao gồm:

S: Tập hợp các trạng thái.

A: Tập hợp các hành động.

P: Hàm xác suất chuyển trạng thái $P(s'|s,a)$, xác định xác suất chuyển từ trạng thái $s$ đến $s'$ khi thực hiện hành động $a$.

R: Hàm phần thưởng $R(s,a,s')$, xác định phần thưởng nhận được khi chuyển từ $s$ đến $s'$ bằng hành động $a$.

γ: Hệ số chiết khấu $(0 \leq \gamma \leq 1)$, xác định tầm quan trọng của phần thưởng tương lai.

2. Thuộc tính Markov
Tính chất Markov chỉ ra rằng xác suất chuyển trạng thái và phần thưởng chỉ phụ thuộc vào trạng thái hiện tại và hành động hiện tại, không phụ thuộc vào lịch sử trước đó.
3. Policy
Một Policy $\pi$ xác định cách tác nhân chọn hành động dựa trên trạng thái hiện tại.

Bao gồm:

Xác suất: $\pi(a|s)$ là xác suất chọn hành động $a$ tại trạng thái $s$.

Xác định: $\pi(s) = a$ là hành động cụ thể được chọn tại trạng thái $s$.

4. Phân loại nhiệm vụ
Nhiệm vụ kết thúc (Episodic): Có trạng thái kết thúc rõ ràng, ví dụ: chơi một ván cờ.

Nhiệm vụ liên tục (Continuing): Không có trạng thái kết thúc, ví dụ: kiểm soát nhiệt độ trong một tòa nhà. 

- Kết luận: 
MDP là mô hình cơ bản để mô tả các bài toán trong học tăng cường.

Hiểu rõ các thành phần của MDP giúp thiết kế và phân tích các thuật toán học tăng cường hiệu quả.

Phân biệt giữa nhiệm vụ kết thúc và liên tục là cần thiết để chọn chiến lược học phù hợp.


