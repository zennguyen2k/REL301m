1. Hàm giá trị trạng thái (V(s))
Đại diện cho giá trị kỳ vọng của việc bắt đầu từ trạng thái $s$ và tuân theo một chính sách $\pi$ cụ thể.
2. Hàm giá trị hành động (Q(s, a))
Đại diện cho giá trị kỳ vọng của việc thực hiện hành động $a$ tại trạng thái $s$ và sau đó tuân theo chính sách $\pi$.
3. Phương trình Bellman
Thiết lập mối quan hệ đệ quy giữa giá trị của một trạng thái và giá trị của các trạng thái kế tiếp.
4. Chính sách tối ưu và giá trị tối ưu
Chính sách tối ưu $\pi^*$ là chính sách mang lại giá trị kỳ vọng cao nhất cho mọi trạng thái.

- Kết luận: 
Hàm giá trị cung cấp thông tin về mức độ "tốt" của một trạng thái hoặc hành động dưới một chính sách cụ thể.

Phương trình Bellman là công cụ quan trọng để tính toán và cập nhật hàm giá trị.

Hiểu rõ mối quan hệ giữa hàm giá trị và chính sách là nền tảng để phát triển các thuật toán học tăng cường hiệu quả.
