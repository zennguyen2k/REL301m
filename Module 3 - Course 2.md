- Mục tiêu:

Hiểu cách áp dụng học tăng cường dựa trên mẫu để kiểm soát chính sách.

Nắm vững các thuật toán Sarsa, Q-learning và Expected Sarsa.

Phân biệt giữa phương pháp học on-policy và off-policy.

Thực hành triển khai các thuật toán này trong môi trường mô phỏng như Cliff World.

- Nội dung chính:

1. Sarsa (State-Action-Reward-State-Action)
Là một thuật toán on-policy, nghĩa là cập nhật chính sách dựa trên hành động thực sự đã chọn theo chính sách hiện tại.

Cập nhật giá trị hành động theo công thức: ![image](https://github.com/user-attachments/assets/99883868-150a-4209-9b4c-b8668b922c29)

2. Q-learning
Là một thuật toán off-policy, nghĩa là cập nhật chính sách dựa trên hành động tối ưu hóa, không nhất thiết phải là hành động đã thực hiện.

Cập nhật giá trị hành động theo công thức: ![image](https://github.com/user-attachments/assets/9f50dd7c-d9de-407d-9898-a50588e43257)

3. Expected Sarsa
Kết hợp giữa Sarsa và Q-learning bằng cách sử dụng kỳ vọng của giá trị hành động tiếp theo theo chính sách hiện tại.

Cập nhật giá trị hành động theo công thức: ![image](https://github.com/user-attachments/assets/044381a6-12e2-4281-9db7-bad737a869f0)

- Kết luận:

Module 3 cung cấp kiến thức sâu sắc về cách sử dụng các thuật toán học tăng cường dựa trên mẫu để kiểm soát chính sách.

Hiểu rõ sự khác biệt giữa các thuật toán và khi nào nên áp dụng từng thuật toán sẽ giúp xây dựng các tác nhân học tăng cường hiệu quả hơn.



