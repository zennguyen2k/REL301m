- Mục tiêu: 
Hiểu cách ước lượng hàm giá trị và chính sách tối ưu chỉ từ trải nghiệm mẫu của tác nhân.

Làm quen với các phương pháp Monte Carlo cho dự đoán và điều khiển.

Phân biệt giữa on-policy và off-policy trong học tăng cường.

Khám phá vấn đề khám phá trong học tăng cường, vượt ra ngoài bài toán bandit.

- Nội dung chính: 
1. Giới thiệu khóa học
Giới thiệu về khóa học, giảng viên và các mục tiêu học tập.
Khuyến khích học viên giới thiệu bản thân trong phần "Meet and Greet".
2. Phương pháp Monte Carlo cho Dự đoán và Điều khiển
Học cách ước lượng hàm giá trị và chính sách tối ưu bằng cách sử dụng chỉ trải nghiệm mẫu từ môi trường.

Giới thiệu về các phương pháp học tăng cường dựa trên mẫu, như Monte Carlo, để học từ tương tác của tác nhân với thế giới, thay vì dựa vào mô hình của thế giới.

Tìm hiểu về các phương pháp on-policy và off-policy cho dự đoán và điều khiển, sử dụng các phương pháp Monte Carlo — phương pháp sử dụng các giá trị trả về mẫu.

Khám phá lại vấn đề khám phá, nhưng tổng quát hơn trong học tăng cường, vượt ra ngoài bandits. 

- Kết luận: 
Chuyển từ các phương pháp học dựa trên mô hình sang học dựa trên trải nghiệm thực tế.

Hiểu rõ sự khác biệt giữa on-policy và off-policy trong học tăng cường.

Khám phá vấn đề khám phá trong học tăng cường, một khía cạnh quan trọng để đảm bảo tác nhân không bị mắc kẹt trong các hành vi không tối ưu.
